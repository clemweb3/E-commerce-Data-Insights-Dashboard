{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a89c0f7",
   "metadata": {},
   "source": [
    "### Initial Setup and Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dd6217",
   "metadata": {},
   "source": [
    "The notebook begins by importing the core libraries used throughout the project. Each has a distinct role in the workflow:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5b1d12",
   "metadata": {},
   "source": [
    "Pandas serves as the main workhorse for cleaning and transforming the dataset. NumPy supports numerical operations that come up during preprocessing, while Matplotlib and Seaborn are reserved for visual checks that help validate assumptions about the data. The imports here already reflect a lightweight BI-oriented pipeline rather than a heavyweight ML environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53ce63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\SHANIA\\Downloads\\E-commerce-Data-Insights-Dashboard\\data\\ecommerce_sales_raw.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d612e4",
   "metadata": {},
   "source": [
    "Viewing the first few rows is an early check. This confirms that the file path is correct, the delimiter is recognized, and no immediate structural issues appear upon load. Since this is a synthetic dataset meant to behave like transactional E-commerce data, the preview lets us validate that the columns align with expectations such as order_id, customer_id, pricing fields, dates, and categorical descriptors.\n",
    "\n",
    "After confirming that the file loads cleanly, the next few calls inspect the basic structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78887cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed7bb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e43c3ce",
   "metadata": {},
   "source": [
    "The dataset contains 34,500 rows and 17 columns, a manageable size suitable for BI dashboards and light predictive work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae7da24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c830bc48",
   "metadata": {},
   "source": [
    "The field names are clean and already analysis-ready, removing the need for renaming or manual parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb14e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be97484",
   "metadata": {},
   "source": [
    "Several key fields are initially typed as object strings, including order_date, which will need explicit conversion later so time-based analysis works correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934bf0f6",
   "metadata": {},
   "source": [
    "### Missing Values, Duplicates, and Initial Data Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4547a4",
   "metadata": {},
   "source": [
    "This section validates the quality of the dataset before performing deeper preprocessing. Since the file originates from a Kaggle project that simulates E-commerce operations, it is expected to be cleaner than real transactional data, although it still mirrors the structure and statistical behavior of real-world sales systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5e273a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6f56be",
   "metadata": {},
   "source": [
    "Missing Values Report\n",
    "\n",
    "•  No nulls detected.\n",
    "\n",
    "•  Expected for synthetic data but simplifies pipeline.\n",
    "\n",
    "•  No imputation needed for BI or ML baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc5131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73743f4",
   "metadata": {},
   "source": [
    "No duplicated records were detected. For a transactional dataset with order_id and customer_id, this suggests the generator ensured uniqueness of each transaction. In practical settings, duplicates would normally arise from ingestion errors or repeated API calls, but for this dataset, uniqueness is guaranteed. The check is still essential because it confirms that aggregation, grouping, or potential time-series breakdowns will not be inflated by accidental repeat entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae4b224",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['order_date'] = pd.to_datetime(df['order_date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1b6063",
   "metadata": {},
   "source": [
    "The date column arrives as an object type, which prevents temporal analysis. Converting it into a datetime type allows the notebook to later extract month, year, seasonality, or forecast horizons. The errors='coerce' parameter ensures that any incorrectly formatted entries would become NaT instead of breaking the pipeline. Even though no errors appear here, using coercion is a defensive habit typical in real business pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a54cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "df[numeric_cols].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7016e2ca",
   "metadata": {},
   "source": [
    "Descriptive statistics help verify whether values fall within plausible ranges. Several observations emerge:\n",
    "\n",
    "• Price values range from 1 to around 2,900, which is plausible for a mixed-category E-commerce platform.\n",
    "\n",
    "• Delivery times range from 3 to 13 days, behaving like typical 3-7 day windows with a few slower deliveries.\n",
    "\n",
    "• Profit margins have negative values, which is meaningful because it can imply discounted or loss-leader transactions rather than errors.\n",
    "\n",
    "• Customer ages range from 18 to 69, a realistic bracket for online shoppers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206de3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numeric_cols].hist(bins=30, edgecolor='black', figsize=(15,10))\n",
    "dpi = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278dd492",
   "metadata": {},
   "source": [
    "Plotting histograms helps detect skewness, heavy tails, or unnatural spikes. Many features show right skew (price, total amount, profit margin). This is typical for E-commerce, where most orders are low-value and high-ticket items form the long tail.\n",
    "\n",
    "Even though the dataset is clean, these distributions help confirm that future predictive work should consider normalization or log-scaling for certain models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74816b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_counts = {}\n",
    "for col in numeric_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = df[(df[col] < (Q1 - 1.5*IQR)) | (df[col] > (Q3 + 1.5*IQR))]\n",
    "    outlier_counts[col] = outliers.shape[0]\n",
    "\n",
    "pd.Series(outlier_counts, name='Potential Outliers')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed23431",
   "metadata": {},
   "source": [
    "Applying the IQR rule shows a notable volume of statistically flagged values.  These figures don’t automatically imply data issues. In many E-commerce contexts, wide value ranges can emerge from several factors such as occasional bulk purchases, high-ticket items, sporadic promotions, or returns that affect computed margins. Since the dataset is a simulation of real activity, these patterns may simply reflect diverse customer behaviors rather than anomalies.\n",
    "\n",
    "Only the outlier counts were computed. No filtering was applied because removing values without a clear business rationale risks distorting spending or operational patterns. At this point, the results are treated as indicators of distribution shape rather than errors. This keeps the dataset aligned with the project’s goal of generating realistic Tableau insights and light predictive exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc8f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = df.select_dtypes(include='object').columns\n",
    "df[categorical_cols].nunique() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d879b267",
   "metadata": {},
   "source": [
    "The cardinality profile looks appropriate:\n",
    "\n",
    "• Thousands of unique customers and products\n",
    "\n",
    "• Only a handful of payment methods and regions\n",
    "\n",
    "• Two values for returned, three for gender\n",
    "\n",
    "This mix aligns with typical retail data and confirms the dataset is structurally suited for segmentation, cohort analysis, and feature engineering later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5cfe11",
   "metadata": {},
   "source": [
    "### Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cef316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Copy original df to avoid contaminating raw version ---\n",
    "df_fe = df.copy()\n",
    "\n",
    "# --- 2. Basic feature engineering ---\n",
    "df_fe['delivery_speed'] = df_fe['delivery_time_days']\n",
    "\n",
    "df_fe['discount_bucket'] = pd.cut(\n",
    "    df_fe['discount'],\n",
    "    bins=[-0.01, 0.01, 0.10, 0.30],\n",
    "    labels=['No Discount', 'Low Discount', 'High Discount']\n",
    ")\n",
    "\n",
    "df_fe['total_cost_ratio'] = df_fe['shipping_cost'] / df_fe['total_amount']\n",
    "\n",
    "df_fe['order_value_segment'] = pd.cut(\n",
    "    df_fe['total_amount'],\n",
    "    bins=[0, 50, 200, 500, df_fe['total_amount'].max()],\n",
    "    labels=['Low Value', 'Mid Value', 'High Value', 'Very High']\n",
    ")\n",
    "\n",
    "df_fe['returned_flag'] = df_fe['returned'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# --- 3. Advanced / signal-based feature engineering (time-aware) ---\n",
    "df_fe = df_fe.sort_values(by='order_date')\n",
    "\n",
    "# Customer-level cumulative features\n",
    "df_fe['past_orders'] = df_fe.groupby('customer_id').cumcount()\n",
    "df_fe['past_returns'] = (\n",
    "    df_fe.groupby('customer_id')['returned_flag']\n",
    "    .cumsum()\n",
    "    .shift(fill_value=0)\n",
    ")\n",
    "\n",
    "# Product-level cumulative return rate\n",
    "df_fe['product_returns_cum'] = (\n",
    "    df_fe.groupby('product_id')['returned_flag']\n",
    "    .cumsum()\n",
    "    .shift(fill_value=0)\n",
    ")\n",
    "df_fe['product_orders_cum'] = df_fe.groupby('product_id').cumcount()\n",
    "df_fe['product_return_rate'] = (\n",
    "    df_fe['product_returns_cum'] / df_fe['product_orders_cum'].replace(0, np.nan)\n",
    ")\n",
    "\n",
    "# --- 3b. Handle NaNs ---\n",
    "df_fe['product_return_rate'] = df_fe['product_return_rate'].fillna(0)\n",
    "df_fe['past_orders'] = df_fe['past_orders'].fillna(0).astype(int)\n",
    "df_fe['past_returns'] = df_fe['past_returns'].fillna(0).astype(int)\n",
    "\n",
    "# --- 4. Extra refined features ---\n",
    "# Recency: days since last order per customer\n",
    "df_fe['days_since_last_order'] = (\n",
    "    df_fe.groupby('customer_id')['order_date']\n",
    "    .diff()\n",
    "    .dt.days\n",
    "    .fillna(9999)  # large value for first order\n",
    ")\n",
    "\n",
    "# Discount × category interaction (categorical combo for modeling)\n",
    "df_fe['discount_category'] = df_fe['discount'].round(2).astype(str) + \"_\" + df_fe['category']\n",
    "\n",
    "# Price per unit\n",
    "df_fe['price_per_unit'] = df_fe['price'] / df_fe['quantity']\n",
    "\n",
    "# Temporal features\n",
    "df_fe['order_month'] = df_fe['order_date'].dt.month\n",
    "df_fe['order_weekday'] = df_fe['order_date'].dt.weekday\n",
    "\n",
    "print(\"Shape after refined feature engineering:\", df_fe.shape)\n",
    "print(\"Sample columns:\", df_fe.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c20877",
   "metadata": {},
   "source": [
    "\n",
    "### **Feature Engineering Overview**\n",
    "\n",
    "The dataset was expanded from 17 to 27 columns, preserving all 34,500 orders and creating **interpretable features that support both dashboards and predictive modeling**. Key engineered features include:\n",
    "\n",
    "* **Delivery speed (`delivery_speed`)** captures order transit time, allowing analysis of whether longer deliveries influence returns.\n",
    "* **Discount bucket (`discount_bucket`)** groups orders into ‘No’, ‘Low’, and ‘High’ discount tiers, helping assess how promotions affect return likelihood.\n",
    "* **Total cost ratio (`total_cost_ratio`)** measures shipping cost relative to order value, highlighting operational stress points that may drive returns.\n",
    "* **Order value segment (`order_value_segment`)** categorizes orders by purchase size, enabling risk and profitability segmentation.\n",
    "* **Return flag (`returned_flag`)** converts returns into a 0/1 target for ML while supporting summary metrics in dashboards.\n",
    "* **Customer-level features (`past_orders`, `past_returns`)** provide historical context, identifying patterns in buyer behavior.\n",
    "* **Product-level feature (`product_return_rate`)** highlights products prone to returns, guiding inventory and operational decisions.\n",
    "* **Temporal features (`order_month`, `order_weekday`)** reveal seasonality and weekday effects on sales and returns.\n",
    "\n",
    "**Impact & questions addressed:** These features allow the business to ask *Which orders or customers are more likely to return products? How do shipping, discounting, or order value affect returns? Which products or periods generate higher risk?* They also provide **interpretable signals for ML models**, enabling accurate prediction of returns while supporting actionable insights in dashboards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081524d8",
   "metadata": {},
   "source": [
    "###  ML Pipeline: Predicting Returns from Order Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca05f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e47b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- 1. Define features ---\n",
    "numeric_features = [\n",
    "    'delivery_speed', 'total_cost_ratio', 'past_orders', 'past_returns',\n",
    "    'product_return_rate', 'customer_age', 'days_since_last_order', 'price_per_unit'\n",
    "]\n",
    "categorical_features = [\n",
    "    'discount_bucket', 'order_value_segment', 'payment_method', 'region',\n",
    "    'customer_gender', 'discount_category'\n",
    "]\n",
    "\n",
    "X = df_fe[numeric_features + categorical_features]\n",
    "y = df_fe['returned_flag']\n",
    "\n",
    "# --- 2. Train/test split (stratified) ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# --- 3. Preprocessing ---\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numeric_features),\n",
    "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- 4. LightGBM Classifier ---\n",
    "clf_lgbm = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', lgb.LGBMClassifier(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=-1,\n",
    "        random_state=42,\n",
    "        class_weight='balanced',   # handles imbalance\n",
    "        scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1])  # ratio of negatives to positives\n",
    "    ))\n",
    "])\n",
    "\n",
    "clf_lgbm.fit(X_train, y_train)\n",
    "y_proba_lgbm = clf_lgbm.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# --- 5. Threshold tuning via PR curve ---\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba_lgbm)\n",
    "f1_scores = (2 * precision * recall) / (precision + recall + 1e-9)\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[max(best_idx - 1, 0)]\n",
    "y_pred_lgbm = (y_proba_lgbm >= best_threshold).astype(int)\n",
    "\n",
    "print(\"LightGBM Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lgbm))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_lgbm))\n",
    "print(\"\\nROC-AUC Score:\", roc_auc_score(y_test, y_proba_lgbm))\n",
    "\n",
    "# --- 6. PR curve visualization ---\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label=f'LGBM PR-AUC = {auc(recall, precision):.2f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve (LightGBM)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# --- 7. Feature importance ---\n",
    "feature_names = numeric_features + list(clf_lgbm.named_steps['preprocessor'].transformers_[1][1].get_feature_names_out(categorical_features))\n",
    "importances = clf_lgbm.named_steps['classifier'].feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"\\nFeature Importance (LightGBM):\")\n",
    "for i in indices[:25]:\n",
    "    print(f\"{feature_names[i]}: {importances[i]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd936c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- Add ML predictions to the feature-engineered dataframe ---\n",
    "\n",
    "# Logistic Regression predictions (probability of return)\n",
    "df_fe['return_prob_lr'] = clf_lr.predict_proba(X)[:,1]\n",
    "\n",
    "# Apply the threshold used in training (0.3)\n",
    "threshold_lr = 0.3\n",
    "df_fe['return_pred_lr'] = (df_fe['return_prob_lr'] > threshold_lr).astype(int)\n",
    "\n",
    "# If Random Forest is trained, use rf (or adjust variable name accordingly)\n",
    "df_fe['return_prob_rf'] = rf.predict_proba(X)[:,1]\n",
    "df_fe['return_pred_rf'] = (df_fe['return_prob_rf'] >= 0.5).astype(int)\n",
    "\n",
    "# --- Export to CSV ---\n",
    "output_folder = r\"C:\\Users\\SHANIA\\Downloads\\E-commerce-Data-Insights-Dashboard\\data\"\n",
    "output_file = \"ecommerce_sales_with_predictions.csv\"\n",
    "output_path = os.path.join(output_folder, output_file)\n",
    "df_fe.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Dataset with predictions saved to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
